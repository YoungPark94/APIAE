{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a tensorlfow implementation for the demonstration of Pendulum experiment of the paper:\n",
    "# \"Adaptive Path-Integral Approach for Representation Learning and Planning of State Space Models\"\n",
    "#\n",
    "# This code trains the APIAE network ans save weights of the network.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation, rc\n",
    "import pickle\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # will use only the first GPU devices\n",
    "plt.rcParams.update({'figure.max_open_warning': 0}) # off matplotlib warning\n",
    "\n",
    "# Fix random seeds\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trainig data\n",
    "file = open(\"pendulum_data.pkl\",'rb')\n",
    "PendulumData = pickle.load(file)\n",
    "Xref = PendulumData[0]\n",
    "Zref = PendulumData[1]\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DynNet(object):\n",
    "    \"\"\" Neural-net for the dynamics model: zdot=dz/dt=f(z) \"\"\"\n",
    "    def __init__(self, intermediate_units, n_z):\n",
    "        self.intermediate_units = intermediate_units\n",
    "        self.Layers = []\n",
    "\n",
    "        # Construct the neural network\n",
    "        for i, unit in enumerate(self.intermediate_units[:-1]):\n",
    "            self.Layers.append(\n",
    "                tf.layers.Dense(units=unit, activation=tf.nn.relu, name='DynLayer' + str(i)))  # fully-connected layer\n",
    "        self.Layers.append(tf.layers.Dense(units=self.intermediate_units[-1], name='DynLayerLast'))\n",
    "\n",
    "        # Below is for the later use of this network\n",
    "        self.z_in = tf.placeholder(tf.float32, (None, n_z))\n",
    "        self.zdot_out = self.compute_zdot(self.z_in)\n",
    "\n",
    "        # Below is for the initialization\n",
    "        self.zdot_ref = tf.placeholder(tf.float32, (None, n_z))\n",
    "        self.loss = tf.reduce_mean(tf.reduce_sum((self.zdot_out - self.zdot_ref) ** 2, axis=-1))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(self.loss)\n",
    "\n",
    "    def compute_zdot(self, z):\n",
    "        # Input :  z=(:,n_z)\n",
    "        # Output: zdot=(:,n_z)\n",
    "        z1dot = z[:, 1:2] # we fixed z_1 dot = z_2 for pendulum example\n",
    "        z2dot = z # z_2 dot = neural_net(z)\n",
    "        for layer in self.Layers:\n",
    "            z2dot = layer(z2dot)\n",
    "        zdot = tf.concat([z1dot, z2dot], axis=1)\n",
    "        return zdot\n",
    "\n",
    "    def initialize(self, sess, z_ref, zdot_ref, minibatchsize=500, training_epochs=5000, display_step=100):\n",
    "        n_data = z_ref.shape[0]\n",
    "        total_batch = int(n_data / minibatchsize)\n",
    "\n",
    "        for epoch in range(training_epochs):\n",
    "            avg_loss = 0\n",
    "            nperm = np.random.permutation(n_data)\n",
    "\n",
    "            # Loop over all batches\n",
    "            for i in range(total_batch):\n",
    "                minibatch_idx = nperm[i * minibatchsize:(i + 1) * minibatchsize]\n",
    "                batch_zs = z_ref[minibatch_idx, :]\n",
    "                batch_zdots = zdot_ref[minibatch_idx, :]\n",
    "\n",
    "                opt, loss = sess.run((self.optimizer, self.loss), feed_dict={self.z_in: batch_zs, self.zdot_ref: batch_zdots})\n",
    "                avg_loss += loss/total_batch\n",
    "\n",
    "            if epoch % display_step == 0:\n",
    "                print(\"Epoch:\", '%04d' % (epoch + 1), \"loss=\", \"{:.9f}\".format(avg_loss))\n",
    "                \n",
    "        print(\"Epoch:\", '%04d' % (epoch + 1), \"loss=\", \"{:.9f}\".format(avg_loss))\n",
    "        \n",
    "class GenNet(object):\n",
    "    \"\"\" Neural-net for the generative model: x=g(z) \"\"\"\n",
    "    def __init__(self, intermediate_units, n_z):\n",
    "        self.intermediate_units = intermediate_units\n",
    "        self.Layers = []\n",
    "\n",
    "        # Construct the neural network\n",
    "        for i, unit in enumerate(self.intermediate_units[:-1]):\n",
    "            self.Layers.append(tf.layers.Dense(units=unit, activation=tf.nn.relu,\n",
    "                                               name='GenLayer' + str(i)))  # fully-connected layer w/ relu\n",
    "        self.Layers.append(tf.layers.Dense(units=self.intermediate_units[-1], name='GenLayerLast'))  # last layer doesn't have relu activation\n",
    "\n",
    "        # Below is for the later use of this network\n",
    "        self.z_in = tf.placeholder(tf.float32, (None, n_z))\n",
    "        self.x_unactivated = self.compute_x(self.z_in)\n",
    "        self.x_out = tf.nn.sigmoid(self.x_unactivated)\n",
    "\n",
    "    def compute_x(self, z):\n",
    "        # Input : z=(:,n_z)\n",
    "        # Output: x=(:,n_x)\n",
    "\n",
    "        x = z[:, 0:1]\n",
    "        for layer in self.Layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class APIAE(object):\n",
    "    \"\"\" APIAE for posterior inference in the latent space\"\"\"\n",
    "    # Data shape : (Batch(B), Samples(L), Time(K), Dim1, Dim2)\n",
    "    # If data is vector, Dim2 = 1\n",
    "    \n",
    "    def __init__(self,R,L,K,dt,n_x,n_z,n_u,ur,lr,isPlanner=False):   \n",
    "        self.R = R # the number of improvements\n",
    "        self.L = L # the number of trajectory sampled\n",
    "        self.K = K # the number of time steps\n",
    "        self.dt = dt # time interval\n",
    "        self.sdt = np.sqrt(dt) # sqrt(dt)\n",
    "        \n",
    "        self.n_x = n_x # dimension of x; observation\n",
    "        self.n_z = n_z # dimension of z; latent space\n",
    "        self.n_u = n_u # dimension of u; control\n",
    "        \n",
    "        self.ur = ur # update rate\n",
    "        self.lr = lr # learning rate\n",
    "        \n",
    "        self.isPlanner=isPlanner # flag whether this network is for the planning or not.\n",
    "            \n",
    "        self.xseq = tf.placeholder(tf.float32, shape=(None,1,self.K,self.n_x,1)) # input sequence of observations\n",
    "        self.B = tf.shape(self.xseq)[0] # the number of batch\n",
    "        \n",
    "        # Define the networks for dynamics and generative model\n",
    "        self.dynNet = DynNet(intermediate_units=[128,1], n_z=self.n_z) # dimension of the last layer is fixed as 1 for the pendulum example.\n",
    "        self.genNet = GenNet(intermediate_units=[128,self.n_x], n_z=self.n_z)\n",
    "        \n",
    "        # Construct PI-net\n",
    "        self._create_network()\n",
    "        \n",
    "        # corresponding optimizer\n",
    "        self._create_loss_optimizer()\n",
    "        \n",
    "        # Initializing the tensor flow variables and saver\n",
    "        self.sess = tf.Session()\n",
    "        self.saver = tf.train.Saver()\n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        # Launch the session\n",
    "        self.sess.run(init)\n",
    "    \n",
    "    def _create_network(self):\n",
    "        self.muhat_list = []\n",
    "        self.museq_list = []\n",
    "        self.Sighat_list = []\n",
    "        self.Lhat_list = []\n",
    "        self.Lhatinv_list = []\n",
    "        self.Linvseq_list = []\n",
    "        self.z0_list = [] \n",
    "        self.dwseq_list = []\n",
    "        self.Kseq_list = []\n",
    "        self.uffseq_list = []\n",
    "        self.zseq_list = []\n",
    "        self.S_list = []\n",
    "        \n",
    "        self._initial_state() # Define and initialize variables\n",
    "        for r in range(self.R):\n",
    "            self._Sampler() # Sampling z0(initial latent state) and dwseq(sequence of dynamic noise)\n",
    "            self._Simulate() # Run dynamics and calculate the cost\n",
    "            self._Update() # Update optimal control sequence & initial state dist.\n",
    "\n",
    "    def _initial_state(self):\n",
    "        # For initial states\n",
    "        if self.isPlanner:\n",
    "            self.mu0 = tf.placeholder(tf.float32, shape=(None,1,1,self.n_z,1)) # input sequence of observations\n",
    "        else:\n",
    "            self.mu0 = tf.zeros((1,1,1,self.n_z,1))\n",
    "        self.Sig0 = tf.reshape(tf.diag([8.,10.]),(1,1,1,self.n_z,self.n_z)) # initialzie witth arbitrary value\n",
    "        self.L0inv = tf.matrix_inverse(tf.cholesky(self.Sig0)) # cholesky lower triangular matrix of Sig0\n",
    "        \n",
    "        muhat = tf.tile(self.mu0, (self.B,1,1,1,1))\n",
    "        Sighat = tf.tile(self.Sig0, (self.B,1,1,1,1))\n",
    "        Lhat = tf.cholesky(Sighat)\n",
    "        \n",
    "        self.Lhat_list.append(Lhat)\n",
    "        self.Lhatinv_list.append(tf.matrix_inverse(Lhat))\n",
    "        self.muhat_list.append(muhat)\n",
    "        self.Sighat_list.append(Sighat)\n",
    "        \n",
    "        # For control input\n",
    "        self.Kseq_list.append(tf.zeros((self.B,1,self.K-1,self.n_u,self.n_z))) # linear feedback term for control policy\n",
    "        self.uffseq_list.append(tf.zeros((self.B,1,self.K-1,self.n_u,1))) # feed forward term for control policy\n",
    "        \n",
    "        self.museq_list.append(tf.zeros((self.B,1,self.K,self.n_z,1)))\n",
    "        self.Linvseq_list.append(tf.eye(self.n_z, batch_shape=(self.B,1,self.K))) \n",
    "        \n",
    "    def _Sampler(self):\n",
    "        # For initial states\n",
    "        muhat = self.muhat_list[-1]        \n",
    "        if self.isPlanner:\n",
    "            z0 = tf.tile(muhat,(1, self.L, 1,1,1))\n",
    "        else:\n",
    "            epsilon_z = tf.random_normal((self.B,self.L,1,self.n_z,1), 0., 1., dtype=tf.float32) # (B,L,1,n_z,1)\n",
    "            Lhat_repeat = tf.tile(self.Lhat_list[-1], (1, self.L, 1,1,1))\n",
    "            z0 = muhat + Lhat_repeat@epsilon_z\n",
    "        self.z0_list.append(z0)   \n",
    "            \n",
    "        # For dynamic noise\n",
    "        epsilon_u = tf.random_normal((self.B, self.L, self.K-1, self.n_u,1), 0., 1., dtype=tf.float32) # sample noise from N(0,1)\n",
    "        dwseq = epsilon_u*self.sdt\n",
    "        self.dwseq_list.append(dwseq)\n",
    "        \n",
    "    def _Simulate(self):\n",
    "        # Load initial states and linear feedback parameters\n",
    "        z0 = self.z0_list[-1]\n",
    "        dwseq = self.dwseq_list[-1]\n",
    "        \n",
    "        museq = self.museq_list[-1]\n",
    "        Linvseq = self.Linvseq_list[-1]\n",
    "        Kseq = self.Kseq_list[-1]\n",
    "        uffseq = self.uffseq_list[-1]\n",
    "\n",
    "        # Reshape variables\n",
    "        museq_repeat = tf.tile(museq, (1, self.L, 1, 1, 1))  # (B,L,K,n_z,1)\n",
    "        Linvseq_repeat = tf.tile(Linvseq, (1,self.L,1,1,1)) # (B,L,K,n_z,n_z)\n",
    "        Kseq_repeat = tf.tile(Kseq, (1,self.L,1,1,1)) # (B,L,K,n_u,n_z)\n",
    "        uffseq_repeat = tf.tile(uffseq, (1,self.L,1,1,1)) # (B,L,K,n_u,1)\n",
    "        \n",
    "        museq_repeat_merge = tf.reshape(museq_repeat,(-1,self.K,self.n_z,1)) # (BL,K,n_z,1)\n",
    "        Linvseq_repeat_merge = tf.reshape(Linvseq_repeat,(-1,self.K,self.n_z,self.n_z)) # (BL,K,n_z,n_z)\n",
    "        Kseq_repeat_merge = tf.reshape(Kseq_repeat,(-1,self.K-1,self.n_u,self.n_z)) # (BL,K-1,n_u,n_z)\n",
    "        uffseq_repeat_merge = tf.reshape(uffseq_repeat,(-1,self.K-1,self.n_u,1)) # (BL,K-1,n_u,1)\n",
    "        dwseq_merge = tf.reshape(dwseq, (-1, self.K-1, self.n_u,1)) # (BL,K-1,n_u,1)\n",
    "\n",
    "        z0_merge = tf.reshape(z0,(-1,self.n_z,1)) # (BL,n_z,1)\n",
    "        zt_merge = z0_merge\n",
    "        zt_merge_list = [zt_merge]\n",
    "        utin_merge_list = []\n",
    "\n",
    "        # Compute optimal control with standardized linear feedback policy\n",
    "        for i in range(0,self.K-1):\n",
    "            temp_val = Linvseq_repeat_merge[:,i,:,:]@(zt_merge-museq_repeat_merge[:,i,:,:])\n",
    "            utin_merge = uffseq_repeat_merge[:,i,:,:] + Kseq_repeat_merge[:,i,:,:]@temp_val\n",
    "            zt_merge = self.Propagate(zt_merge, utin_merge + dwseq_merge[:,i,:,:]/self.dt)\n",
    "            zt_merge_list.append(zt_merge)\n",
    "            utin_merge_list.append(utin_merge)\n",
    "\n",
    "        # Reshape variables\n",
    "        uinseq = tf.reshape(tf.stack(utin_merge_list, axis=1),(-1,self.L,self.K-1,self.n_u,1)) # (B,L,K-1,n_u,1)\n",
    "\n",
    "        x_repeat_merge = tf.reshape(tf.tile(self.xseq, (1,self.L,1,1,1)), (-1, self.n_x, 1)) # (BLK,n_x,1)\n",
    "        zseq = tf.reshape(tf.stack(zt_merge_list, axis=1),(-1,self.L,self.K,self.n_z,1)) # (B,L,K,n_z,1)\n",
    "        zseq_merge = tf.reshape(zseq, (-1, self.n_z, 1)) # (BLK, n_x,1)\n",
    "\n",
    "        #  Compute cost\n",
    "        ss = self.state_cost(x_repeat_merge, zseq_merge) # state cost: (BLK,1,1)\n",
    "        sc = self.control_cost(uinseq) # control cost: (B,L,1,1,1)\n",
    "        \n",
    "        if self.isPlanner:\n",
    "            ss = self.K*tf.reshape(ss, (self.B,self.L,self.K,1,1))[:,:,-1:,:,:] # (B,L,1,1,1)\n",
    "            self.S_list.append(ss + sc) #(B,L,1,1,1)\n",
    "        else:\n",
    "            ss = tf.reduce_sum(tf.reshape(ss, (self.B,self.L,self.K,1,1)), axis=2, keep_dims=True) # (B,L,1,1,1)\n",
    "            si = self.initial_cost() # Initial cost: (B,L,1,1,1)\n",
    "            self.S_list.append(ss + sc + si) # (B,L,1,1,1)\n",
    "        self.zseq_list.append(zseq) # (B,L,K,n_z,1)\n",
    "        \n",
    "    def _Update(self):\n",
    "        # Load variables\n",
    "        S = self.S_list[-1]  # (B,L,1,1,1)\n",
    "        zseq = self.zseq_list[-1]  # (B,L,K,n_z,1)\n",
    "        dwseq = self.dwseq_list[-1]  # (B,L,K-1,n_u,1)\n",
    "        Kseq = self.Kseq_list[-1]  # (B,1,K-1,n_u,n_z)\n",
    "        uffseq = self.uffseq_list[-1]  # (B,1,K-1,n_u,1)\n",
    "        Linvseq_cur = self.Linvseq_list[-1][:,:,:-1,:,:]  # (B,L,K-1,n_z,n_z)\n",
    "        museq_cur = self.museq_list[-1][:,:,:-1,:,:]  # (B,L,K-1,n_z,1)\n",
    "\n",
    "        # Compute the weight, alpha = (B,L,1,1,1)\n",
    "        Smin = tf.reduce_min(S, axis=1, keep_dims=True)\n",
    "        alpha = tf.exp(Smin-S) # to avoid positive large alpha\n",
    "        norm = tf.reduce_sum(alpha, axis=1, keep_dims=True)\n",
    "        alpha = alpha/norm\n",
    "        self.alpha_constant = tf.stop_gradient(alpha[:,:,0,0,0])  # clone alpha, but stop gradient\n",
    "\n",
    "        # Compute mean and Cov. of L trajectories\n",
    "        museq_next = tf.reduce_sum(alpha*zseq, axis=1, keep_dims=True)  # (B,1,K,n_z,1)\n",
    "\n",
    "        temp = (zseq - museq_next)  # (B,L,K,n_z,1)\n",
    "        tempT = tf.transpose(temp, perm=(0, 1, 2, 4, 3))  # (B,L,K,1,n_z)\n",
    "        offset = .01 * tf.eye(self.n_z, batch_shape=(1, 1, 1))\n",
    "        Sigseq_next = offset + tf.reduce_sum(alpha*(temp@tempT), axis=1, keep_dims=True)  # (B,1,K,n_z,n_z)\n",
    "\n",
    "        Lseq_next = tf.cholesky(Sigseq_next)\n",
    "        Linvseq_next = tf.matrix_inverse(Lseq_next)\n",
    "        \n",
    "        # Save variables\n",
    "        self.museq_list.append(museq_next)\n",
    "        self.Linvseq_list.append(Linvseq_next)\n",
    "        self.muhat_list.append(museq_next[:, :, :1, :, :])\n",
    "        self.Lhat_list.append(Lseq_next[:, :, :1, :, :])\n",
    "        self.Lhatinv_list.append(Linvseq_next[:, :, :1, :, :])\n",
    "        \n",
    "        # Compute optimal control policy\n",
    "        zseq = zseq[:, :, :-1, :, :]  # (B,L,K-1,n_z,1)\n",
    "        museq_next = museq_next[:, :, :-1, :, :]  # (B,L,K-1,n_z,1)\n",
    "        Lseq_next = Lseq_next[:, :, :-1, :, :]  # (B,L,K-1,n_z,n_z)\n",
    "        Linvseq_next = Linvseq_next[:, :, :-1, :, :]  # (B,L,K-1,n_z,n_z)\n",
    "\n",
    "        bstarseq = uffseq + Kseq @ (Linvseq_cur @ (museq_next - museq_cur)) + self.ur * tf.reduce_sum(alpha * dwseq,\n",
    "                                                                                                    axis=1,\n",
    "                                                                                                    keep_dims=True) / self.dt  # (B,1,1,n_u,1)\n",
    "        temp = tf.transpose(zseq - museq_next, perm=(0, 1, 2, 4, 3))\n",
    "        LinvseqT = tf.transpose(Linvseq_next, perm=(0, 1, 2, 4, 3))\n",
    "        astarseq = Kseq @ Linvseq_cur @ Lseq_next + self.ur * tf.reduce_sum(alpha * (dwseq @ temp), axis=1,\n",
    "                                                                            keep_dims=True) @ LinvseqT / self.dt\n",
    "\n",
    "        # Save variables\n",
    "        self.Kseq_list.append(astarseq)\n",
    "        self.uffseq_list.append(bstarseq)\n",
    "    \n",
    "    def _create_loss_optimizer(self):\n",
    "        S = tf.reduce_sum(self.alpha_constant*self.S_list[-1][:,:,0,0,0], axis = 1) # (B,L)\n",
    "        self.cost_batch = tf.reduce_mean(S) # (scalar)\n",
    "    \n",
    "        # Use ADAM optimizer\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.cost_batch)\n",
    "        \n",
    "    def Propagate(self, zt, ut):\n",
    "        \"\"\"Simulate one-step forward\"\"\"\n",
    "        # Input: zt=(BL,n_z,1), ut=(BL,n_u,1)\n",
    "        # Output: znext=(BL,n_z,1)\n",
    "        zt_squeeze = zt[:,:,0]\n",
    "        zdott_squeeze = self.dynNet.compute_zdot(zt_squeeze)\n",
    "        zdott = tf.expand_dims(zdott_squeeze, axis=-1)\n",
    "        \n",
    "        znext = zt + zdott*self.dt + tf.concat([tf.zeros_like(ut),ut],axis=1)*self.dt\n",
    "        return znext\n",
    "    \n",
    "    def initial_cost(self): \n",
    "        \"\"\"Compute the cost of initial state\"\"\"\n",
    "        z0 = self.z0_list[-1] # (B,L,1,n_z,1) \n",
    "        mu0 = self.mu0 # (1,1,1,n_z,1) \n",
    "        L0inv = tf.tile(self.L0inv, (self.B,self.L,1,1,1)) # (B,L,1,n_z,n_z) \n",
    "        muhat = self.muhat_list[-1] # (B,1,1,n_z,1) \n",
    "        Lhatinv = tf.tile(self.Lhatinv_list[-1], (1,self.L,1,1,1)) # (B,L,1,n_z,n_z)    \n",
    "        S0 = -(Lhatinv@(z0-muhat))**2 + (L0inv@(z0-mu0))**2 # (B,L,1,n_z,1)\n",
    "        \n",
    "        return 0.5 * tf.reduce_sum(S0,axis=-2, keep_dims=True) # (B,L,1,1,1) \n",
    "    \n",
    "    def control_cost(self, uinseq):\n",
    "        \"\"\"Compute the cost of control input\"\"\"\n",
    "        dwseq = self.dwseq_list[-1] \n",
    "        uTu = tf.reduce_sum(uinseq**2, axis=-2, keep_dims=True) # (B,L,K,1,1) \n",
    "        uTdw = tf.reduce_sum(uinseq*dwseq, axis=-2, keep_dims=True) # (B,L,K,1,1) \n",
    "        \n",
    "        return tf.reduce_sum(uTu*0.5*self.dt + uTdw, axis=2, keep_dims=True)\n",
    "    \n",
    "    def state_cost(self,xt_true,zt): # shape of inputs: (BLK, Dim, 1)\n",
    "        \"\"\"Compute the log-likelihood of observation xt given latent zt\"\"\"        \n",
    "        xt = tf.expand_dims(self.genNet.compute_x(zt[:,:,0]),axis=-1)\n",
    "\n",
    "        cost = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=xt_true, logits=xt), axis=1, keep_dims=True)\n",
    "        return cost*self.dt\n",
    "        \n",
    "    def partial_fit(self, X):\n",
    "        \"\"\"Train model based on mini-batch of input data, and return the cost of mini-batch.\"\"\"\n",
    "        opt, cost, zseq_list, museq_list = self.sess.run((self.optimizer, self.cost_batch, self.zseq_list,self.museq_list), \n",
    "                                  feed_dict={self.xseq: X})\n",
    "        return cost, zseq_list, museq_list\n",
    "    \n",
    "    def saveWeights(self, filename=\"weights.pkl\"):\n",
    "        \"\"\"Save the weights of neural networks\"\"\"\n",
    "        weights = {}\n",
    "        for i, layer in enumerate(self.dynNet.Layers):\n",
    "            weights['d_w'+str(i)] = self.sess.run(layer.weights)\n",
    "\n",
    "        for i, layer in enumerate(self.genNet.Layers):\n",
    "            weights['g_w'+str(i)] = self.sess.run(layer.weights)    \n",
    "\n",
    "        filehandler = open(filename,\"wb\")\n",
    "        pickle.dump(weights,filehandler)\n",
    "        filehandler.close()\n",
    "\n",
    "        print('weight saved in '+filename)\n",
    "        return weights\n",
    "\n",
    "    def restoreWeights(self, filename=\"weights.pkl\"):\n",
    "        \"\"\"Load the weights of neural networks\"\"\"\n",
    "        filehandler = open(filename,\"rb\")\n",
    "        weights = pickle.load(filehandler)\n",
    "        filehandler.close()\n",
    "\n",
    "        for i, layer in enumerate(self.dynNet.Layers):\n",
    "            for j, w in enumerate(layer.weights):\n",
    "                self.sess.run(tf.assign(w, weights['d_w'+str(i)][j]))\n",
    "\n",
    "        for i, layer in enumerate(self.genNet.Layers):\n",
    "            for j, w in enumerate(layer.weights):\n",
    "                self.sess.run(tf.assign(w, weights['g_w'+str(i)][j]))  \n",
    "\n",
    "        print('weight restored from '+filename)\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build APIAE\n",
    "params = dict(n_x = 16**2, # dimension of x; observation\n",
    "n_z = 2, # dimension of z; latent space\n",
    "n_u = 1, # dimension of u; control\n",
    "\n",
    "K = 10, # the number of time steps\n",
    "L = 50, # the number of trajectory sampled\n",
    "R = 3,# the number of improvements\n",
    "\n",
    "dt = .1, # time interval\n",
    "ur = .3, # update rate \n",
    "lr = 0.001 # learning rate\n",
    "    )\n",
    "apiae = APIAE(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dynamics network with zdot(t) = Az(t), where A = [[0, -g], [1, -b]]\n",
    "theta_initials = 0.5*np.pi*np.random.normal(loc=0.0, scale=1.0, size=(5000,1)) \n",
    "omega_initials = np.pi*np.random.normal(loc=0.0, scale=1.0, size=(5000,1))\n",
    "Zref_init = np.concatenate([theta_initials, omega_initials], axis = 1)\n",
    "\n",
    "g = 5.\n",
    "b = 5.\n",
    "A_lin = np.array([[0., -g], [1., -b]])\n",
    "Zdot_ref = Zref_init.dot(A_lin)\n",
    "\n",
    "apiae.dynNet.initialize(apiae.sess, Zref_init, Zdot_ref, minibatchsize=500, training_epochs=500, display_step=100)\n",
    "\n",
    "# Show initialized network\n",
    "T_test = 10\n",
    "dt_test = 0.1\n",
    "Z_test = np.zeros((5000,T_test,2))\n",
    "Z_test[:,0,:] = Zref_init\n",
    "for t in range(T_test-1):\n",
    "    dz = apiae.sess.run(apiae.dynNet.zdot_out,feed_dict={apiae.dynNet.z_in:Z_test[:,t,:]})*dt_test\n",
    "    Z_test[:,t+1,:] = Z_test[:,t,:] + dz\n",
    "    \n",
    "plt.close()\n",
    "plt.figure()\n",
    "for b in range(0,Z_test.shape[0],100):\n",
    "    plt.plot(Z_test[b,:,0],Z_test[b,:,1])\n",
    "    plt.plot(Z_test[b,0,0],Z_test[b,0,1],'k.')\n",
    "plt.grid()\n",
    "plt.xlabel('z1')\n",
    "plt.ylabel('z2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "NBatch = 100\n",
    "K = params['K']\n",
    "dt = params['dt']\n",
    "n_x = params['n_x']\n",
    "n_z = params['n_z']\n",
    "t_span = np.linspace(0,(K-1)*dt,K)\n",
    "training_epochs = 20000\n",
    "minibatchsize = np.minimum(250,NBatch)\n",
    "\n",
    "# Training cycle\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(NBatch/minibatchsize)\n",
    "    nperm = np.random.permutation(NBatch)\n",
    "    \n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        minibatch_idx = nperm[i*minibatchsize:(i+1)*minibatchsize]\n",
    "        batch_xs = Xref[minibatch_idx,:,:,:,:]\n",
    "        \n",
    "        # Fit training using batch data\n",
    "        cost,zseq,museq = apiae.partial_fit(batch_xs)\n",
    "        \n",
    "        # Compute average loss\n",
    "        avg_cost += cost\n",
    "        \n",
    "    # Display logs per 10 epoch step\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch:\", '%04d' % (epoch+1), \n",
    "              \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "    \n",
    "    # Display trainig results and save weights per 100 epoch step   \n",
    "    if epoch % 100 == 0:\n",
    "        filename = './weights/weights_'+str(epoch)+'.pkl'\n",
    "        apiae.saveWeights(filename)\n",
    "            \n",
    "        r = params['R']-1\n",
    "        \n",
    "        ims_total = []\n",
    "        K_test = 10\n",
    "        dt_test = 0.1\n",
    "        Z_test = np.zeros((5000,K_test,2))\n",
    "        Z_test[:,0,:] = Zref_init\n",
    "        for t in range(T_test-1):\n",
    "            dz = apiae.sess.run(apiae.dynNet.zdot_out,feed_dict={apiae.dynNet.z_in:Z_test[:,t,:]})*dt_test\n",
    "            Z_test[:,t+1,:] = Z_test[:,t,:] + dz\n",
    "\n",
    "        plt.close()\n",
    "        plt.figure()\n",
    "        for b in range(0,Z_test.shape[0],10):\n",
    "            plt.plot(Z_test[b,:,0],Z_test[b,:,1])\n",
    "            plt.plot(Z_test[b,0,0],Z_test[b,0,1],'k.')\n",
    "        plt.grid()\n",
    "        plt.xlabel('z1')\n",
    "        plt.xlabel('z2')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
